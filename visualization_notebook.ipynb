{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf = yaml.safe_load(open(\"vis_config.yml\"))\n",
    "\n",
    "img_dir = Path(conf[\"paths\"][\"image_dir\"]).resolve()\n",
    "results_dir = Path(conf[\"paths\"][\"results_dir\"]).resolve()\n",
    "csv_results_dir = Path(conf[\"paths\"][\"csv_results_dir\"]).resolve()\n",
    "\n",
    "image_conf = conf['image']\n",
    "input_shape = (image_conf[\"height\"], image_conf[\"width\"], image_conf[\"channels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_header = ['Model', 'EvalMethod', \"EvalParam\", 'AtribMethod', 'n', 'Result']\n",
    "\n",
    "df = pd.read_csv(csv_results_dir / \"resnet_vgg_output_file.csv\")\n",
    "df_train = pd.read_csv(csv_results_dir / \"train_comp_output_file.csv\")\n",
    "df_lrponly = pd.read_csv(csv_results_dir / \"resnet_lrponly_output_file.csv\")\n",
    "#df_rand = pd.read_csv(csv_results_dir / \"rand_output_file.csv\")\n",
    "df_mobile = pd.read_csv(csv_results_dir / \"mobilenet_output_file.csv\")\n",
    "df_selectivity = pd.read_csv(csv_results_dir / \"selectivity_output_file.csv\")\n",
    "df_sens = pd.read_csv(csv_results_dir / \"sens_output_file.csv\")\n",
    "\n",
    "\n",
    "#df = df[df.EvalMethod != \"LimeWrapper(Rule.IDENTITY_RULE)\"].append(df_lime)\n",
    "#df = df[df.AtribMethod != \"RandomAttribution(None)\"].append(df_rand)\n",
    "\n",
    "df = pd.concat([df,df_mobile])\n",
    "df = df.query(\"not (EvalMethod=='RemoveBestPixel' & AtribMethod=='LimeWrapper(Rule.IDENTITY_RULE)')\"\n",
    "    ).append(df_selectivity.query(\"EvalMethod=='RemoveBestPixel'\"))\n",
    "df = df.query(\"not (EvalMethod=='BlurBestPixel' & AtribMethod=='LimeWrapper(Rule.IDENTITY_RULE)')\"\n",
    "    ).append(df_selectivity.query(\"EvalMethod=='BlurBestPixel'\"))\n",
    "    \n",
    "df = df.query(\"not (EvalMethod=='SensitivityN')\"\n",
    "    ).append(df_sens.query(\"EvalMethod=='SensitivityN'\"))\n",
    "\n",
    "#df = df.query(\"Model=='vgg19' | Model=='resnet50' | Model=='mobilenet'\")\n",
    "#df = df.query(\"Model=='custom_custom_model_resnet50_02' | Model=='custom_custom_model_resnet50_04' | Model=='custom_custom_model_resnet50_06' | Model=='custom_custom_model_resnet50_08' | Model=='custom_custom_model_resnet50_10'\")\n",
    "\n",
    "df = pd.concat([df,df_train])\n",
    "\n",
    "del df['EvalParam']\n",
    "df = df[df.AtribMethod != \"RandomAttribution(None)\"]\n",
    "df=df.reset_index()\n",
    "\n",
    "\n",
    "markers = {\n",
    "    \"GradientBased(Rule.GRAD_ONLY)\": \".\",\n",
    "    \"GradientBased(Rule.GRAD_X_INPUT)\": \"s\",\n",
    "    \"GradientBased(Rule.INTEGRATED_GRAD)\": \"X\",\n",
    "    \"GradientBased(Rule.RANDOM_BASELINE_INTEGRATED_GRAD)\": \"D\",\n",
    "    \"LRP(Rule.Z_RULE)\": \"P\",\n",
    "    \"LimeWrapper(Rule.IDENTITY_RULE)\": \"*\",\n",
    "    \"RandomAttribution(None)\": \"|\",\n",
    "#}\n",
    "#markers = {\n",
    "    \"LRP(Rule.HEURISTIC_RULE)\": \".\",\n",
    "    \"LRP(Rule.OMEGA_RULE)\": \"s\",\n",
    "    \"LRP(Rule.IDENTITY_RULE)\": \"X\",\n",
    "    \"LRP(Rule.EPSILON_RULE)\": \"D\",\n",
    "    \"LRP(Rule.Z_RULE)\": \"P\"\n",
    "}\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SensN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.EvalMethod == \"SensitivityN\"]\n",
    "\n",
    "\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    \n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, marker=markers[str(name[1])],markersize=5, label=str(name[1]))\n",
    "\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.8),fontsize=20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RemoveBestPixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.EvalMethod == \"RemoveBestPixel\"]\n",
    "df_filtered.EvalMethod = \"Selectivity(Zeros,Squares)\"\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, marker=markers[str(name[1])],markersize=10, label=str(name[1]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-1),fontsize=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RemoveBestSuperixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.EvalMethod == \"RemoveBestSuperpixel\"]\n",
    "df_filtered.EvalMethod = \"Selectivity(Zeros,Superpixels)\"\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, marker=markers[str(name[1])],markersize=10, label=str(name[1]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlurBestPixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.EvalMethod == \"BlurBestPixel\"]\n",
    "df_filtered.EvalMethod = \"Selectivity(Mean,Squares)\"\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, marker=markers[str(name[1])],markersize=10, label=str(name[1]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlurBestSuperpixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.EvalMethod == \"BlurBestSuperpixel\"]\n",
    "df_filtered.EvalMethod = \"Selectivity(Mean,Superpixels)\"\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, marker=markers[str(name[1])],markersize=10, label=str(name[1]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IROF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.EvalMethod == \"IROF_Remove\"]\n",
    "df_filtered.EvalMethod = \"IROF(Zeros)\"\n",
    "#df_filtered.Result =  df_filtered.n  -df_filtered.Result + 1\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, marker=markers[str(name[1])],markersize=10, label=str(name[1]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.EvalMethod == \"IROF_Blur\"]\n",
    "df_filtered.EvalMethod = \"IROF(Mean)\"\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, marker=markers[str(name[1])],markersize=10, label=str(name[1]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.AtribMethod.str.contains(\"Lime\")]\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, label=str(name[0]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.AtribMethod.str.contains(\"LRP\")]\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(20)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, label=str(name[0]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradBased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df\n",
    "df_filtered = df_filtered[df_filtered.AtribMethod.str.contains(\"GRAD\")]\n",
    "\n",
    "model_grouped = df_filtered.groupby('Model')\n",
    "\n",
    "fig, axs = plt.subplots(len(model_grouped))\n",
    "fig.set_figheight(30)\n",
    "fig.set_figwidth(30)\n",
    "for i, (model_name, model_g) in enumerate(model_grouped):\n",
    "    axs[i].set_title(str(model_name))\n",
    "    for name, g in model_g.groupby(['EvalMethod', 'AtribMethod']):\n",
    "        axs[i].plot(g.n, g.Result, label=str(name[0]))\n",
    "    axs[i].axhline(color='black')\n",
    "    axs[i].set_xlabel(\"Number of Iterations\")\n",
    "    axs[i].set_ylabel(\"Evaluation Score\")\n",
    "axs[-1].legend(loc=(0,-0.9),fontsize=20)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[df.AtribMethod != \"RandomAttribution(None)\"]\n",
    "\n",
    "df_list = []\n",
    "for eval_method in list(new_df.EvalMethod.unique()):\n",
    "    temp = new_df[new_df.EvalMethod == eval_method]\n",
    "    temp_idx = temp.groupby(['Model', 'EvalMethod', 'AtribMethod'])['n'].idxmax()\n",
    "    \n",
    "    df_list.append(temp.loc[temp_idx])\n",
    "new_df = pd.concat(df_list, ignore_index=True)\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import researchpy as rp\n",
    "\n",
    "rp.summary_cat(new_df[['Model', 'EvalMethod', 'AtribMethod']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "temp_df = new_df\n",
    "\n",
    "query_str = '&'.join([\n",
    "    f\"Model!='{m}'\" \n",
    "    for m in [\n",
    "        'vgg19', 'resnet50', 'mobilenet'\n",
    "        ]])\n",
    "temp_df = temp_df.query(query_str)\n",
    "\n",
    "print(\"All\")\n",
    "\n",
    "model = ols('FinalScore ~ C(Model)', data=temp_df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "display(anova_table)\n",
    "\n",
    "for q in temp_df.EvalMethod.unique():\n",
    "    print(q)\n",
    "    temp_df_part = temp_df.query(f\"EvalMethod=='{q}'\")\n",
    "\n",
    "    model = ols('FinalScore ~ C(Model)', data=temp_df_part).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    display(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = new_df\n",
    "\n",
    "\n",
    "query_str = '|'.join([\n",
    "    f\"Model=='{m}'\" \n",
    "    for m in [\n",
    "        'vgg19', 'resnet50', 'mobilenet'\n",
    "        ]])\n",
    "temp_df = temp_df.query(query_str)\n",
    "\n",
    "print(\"All\")\n",
    "model = ols('FinalScore ~ C(Model)', data=temp_df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "display(anova_table)\n",
    "\n",
    "for q in temp_df.EvalMethod.unique():\n",
    "    print(q)\n",
    "    temp_df_part = temp_df.query(f\"EvalMethod=='{q}'\")\n",
    "\n",
    "    model = ols('FinalScore ~ C(Model)', data=temp_df_part).fit()\n",
    "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "    display(anova_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = new_df\n",
    "\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "\n",
    "\n",
    "\n",
    "temp_df = temp_df.replace(['custom_custom_model_resnet50_02'],'Custom ResNet50 (2 epoch)')\n",
    "temp_df = temp_df.replace(['custom_custom_model_resnet50_04'],'Custom ResNet50 (4 epoch)')\n",
    "temp_df = temp_df.replace(['custom_custom_model_resnet50_06'],'Custom ResNet50 (6 epoch)')\n",
    "temp_df = temp_df.replace(['GradientBased(Rule.GRAD_ONLY)'],'Gradients Only')\n",
    "temp_df = temp_df.replace(['GradientBased(Rule.GRAD_X_INPUT)'],'Gradient x Input')\n",
    "temp_df = temp_df.replace(['GradientBased(Rule.INTEGRATED_GRAD)'],'Integrated Gradients')\n",
    "temp_df = temp_df.replace(['GradientBased(Rule.RANDOM_BASELINE_INTEGRATED_GRAD)'],'Random Baseline Integrated Gradients')\n",
    "temp_df = temp_df.replace(['LRP(Rule.Z_RULE)'],'LRP(Z Rule)')\n",
    "temp_df = temp_df.replace(['LimeWrapper(Rule.IDENTITY_RULE)'], 'LIME')\n",
    "temp_df = temp_df.replace(['SensitivityN'], 'Sensitivity-N')\n",
    "temp_df = temp_df.replace(['IROF_Remove'], 'IROF(Zeros)')\n",
    "temp_df = temp_df.replace(['IROF_Blur'], 'IROF(Mean)')\n",
    "temp_df = temp_df.replace(['RemoveBestPixel'], 'Selectivity(Zeros, Squares)')\n",
    "temp_df = temp_df.replace(['BlurBestPixel'], 'Selectivity(Mean, Squares)')\n",
    "temp_df = temp_df.replace(['RemoveBestSuperpixel'], 'Selectivity(Zeros, Superpixels)')\n",
    "temp_df = temp_df.replace(['BlurBestSuperpixel'], 'Selectivity(Mean, Superpixels)')\n",
    "\n",
    "temp_df = temp_df.pivot_table(\n",
    "        values='FinalScore', \n",
    "        index=['Model', 'AtribMethod'], \n",
    "        columns='EvalMethod',\n",
    "        sort=False)\n",
    "\n",
    "\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tukey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "temp_df = new_df\n",
    "\n",
    "query_str = '|'.join([\n",
    "    f\"Model=='{m}'\" \n",
    "    for m in [\n",
    "        'vgg19', 'resnet50', 'mobilenet'\n",
    "        ]])\n",
    "temp_df = temp_df.query(query_str)\n",
    "\n",
    "print(\"All\")\n",
    "temp_df_part = temp_df.query(f\"EvalMethod=='{q}'\")\n",
    "tukey = pairwise_tukeyhsd(endog=temp_df['FinalScore'],\n",
    "                        groups=temp_df['Model'],\n",
    "                        alpha=0.05)\n",
    "\n",
    "tukey.plot_simultaneous()\n",
    "\n",
    "display(tukey.summary())\n",
    "\n",
    "\n",
    "for q in temp_df.EvalMethod.unique():\n",
    "    print(q)\n",
    "    temp_df_part = temp_df.query(f\"EvalMethod=='{q}'\")\n",
    "    tukey = pairwise_tukeyhsd(endog=temp_df_part['FinalScore'],\n",
    "                            groups=temp_df_part['Model'],\n",
    "                            alpha=0.05)\n",
    "\n",
    "    tukey.plot_simultaneous()\n",
    "\n",
    "    display(tukey.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "temp_df = new_df\n",
    "\n",
    "query_str = '&'.join([\n",
    "    f\"Model!='{m}'\" \n",
    "    for m in [\n",
    "        'vgg19', 'resnet50', 'mobilenet'\n",
    "        ]])\n",
    "temp_df = temp_df.query(query_str)\n",
    "\n",
    "print(\"All\")\n",
    "tukey = pairwise_tukeyhsd(endog=temp_df['FinalScore'],\n",
    "                        groups=temp_df['Model'],\n",
    "                        alpha=0.05)\n",
    "\n",
    "tukey.plot_simultaneous()\n",
    "\n",
    "display(tukey.summary())\n",
    "\n",
    "\n",
    "for q in temp_df.EvalMethod.unique():\n",
    "    print(q)\n",
    "    temp_df_part = temp_df.query(f\"EvalMethod=='{q}'\")\n",
    "    tukey = pairwise_tukeyhsd(endog=temp_df_part['FinalScore'],\n",
    "                            groups=temp_df_part['Model'],\n",
    "                            alpha=0.05)\n",
    "\n",
    "    tukey.plot_simultaneous()\n",
    "\n",
    "    display(tukey.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = new_df\n",
    "temp_df.boxplot(column=[\"FinalScore\"], by=[\"Model\"], figsize=(15,20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b7fcf4da149b49c5acad23f67297d346e1591974858b5cc11d8f78d5724a8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
